\documentclass[12pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Probabilistic Inference for Scattering Data Analysis}
\author{Chi-Huan Tung}

\begin{document}

\maketitle

% \section{Introduction}
This document presents a unified framework for applying probabilistic inference to extract physically meaningful quantities from experimental measurements. The methodology is rooted in linear algebra, leveraging matrix operations to propagate uncertainties and recover latent variables from noisy or incomplete data. Emphasis is placed on scenarios where the quantities of interest are not directly measurable but can be inferred through well-defined forward or inverse linear operators. To demonstrate the versatility of this approach, we present several case studies drawn from real-world scattering experiments, including Small-Angle Neutron Scattering (SANS), Neutron Spin Echo (NSE), and X-ray Photon Correlation Spectroscopy (XPCS). We further explore the application of indirect inference techniques to challenging problems such as symmetry-enforced reconstruction, deconvolution, chromatic aberrations, and extremely low detector counts. These developments not only enhance data interpretability, but also open new possibilities for instrument design by enabling systematic exploration of trade-offs between measurement accuracy and experimental efficiency.

\section{Regression Analysis}

Regression analysis provides a systematic framework for estimating unknown physical parameters by aligning experimental measurements with predictions generated by a parameterized model. This process is typically formulated as an optimization problem:
\begin{equation}
    \Lambda = \operatorname*{argmin}_\Lambda \mathcal{L}(X, \Lambda),
\end{equation}
where \( \Lambda \) denotes the model parameters, and the loss function \( \mathcal{L}(X, \Lambda) \) quantifies the discrepancy between the measured data \( Y(X) \) and the model output \( f(X, \Lambda) \):
\begin{equation}
    \mathcal{L}(X, \Lambda) = \sum_X \left\lVert Y(X) - f(X, \Lambda) \right\rVert^2.
\end{equation}

The parameters \( \Lambda \) are iteratively adjusted through a curve-fitting procedure to minimize the loss function. This enables two key outcomes: (1) the inferred parameters \( \Lambda \) may correspond to physically meaningful quantities, and (2) the model \( f(X, \Lambda) \) can interpolate or smooth the data, allowing estimation in regions where direct measurements are unavailable and enhancing numerical robustness.

In practice, however, the functional form \( f \) that relates a low-dimensional parameter set \( \Lambda \) to the observable \( Y \) is often unknown, inaccessible, or too complex to specify analytically. Moreover, experimental observations are inevitably corrupted by noise. As a result, the idealized relationship
\begin{equation}
    Y = f(X, \Lambda)
\end{equation}
is generally violated. A more realistic formulation accounts for measurement uncertainty via an additive noise term:
\begin{equation}
    Y = f(X, \Lambda) + \eta(X),
    \label{eq:noisy_fit}
\end{equation}
where \( \eta(X) \) denotes the noise, typically modeled as a random variable informed by the statistical properties of the measurement process.

Given these challenges, the goal of data analysis becomes the recovery of a physically meaningful estimate of the latent parameters \( \Lambda \) from noisy observations \( Y \), under the following conditions:  
(1) the relationship between \( \Lambda \) and \( Y \) can be approximated by a linear operator; and  
(2) the observation noise is Gaussian-distributed and characterized by a known covariance structure.  

A principled and flexible solution is to adopt a \textbf{probabilistic inference framework}, which enables uncertainty-aware estimation by incorporating prior knowledge about the system and explicitly modeling the effects of measurement noise.

\section{Probabilistic Description of Regression Loss}
To motivate the probabilistic framework, we begin with a simple example: the one-dimensional \textbf{Gaussian (normal) distribution}. A scalar random variable \( y \in \mathbb{R} \) is said to follow a normal distribution with mean \( \mu \in \mathbb{R} \) and variance \( \sigma^2 > 0 \), denoted as
\begin{equation}
    y \sim \mathcal{N}(\mu, \sigma^2),
\end{equation}
if its probability density function is given by:
\begin{equation}
    p(y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(y - \mu)^2}{2\sigma^2} \right).
\end{equation}

This distribution describes the likelihood of observing different values of \( y \), assuming that values closer to the mean \( \mu \) are more probable, and the spread is governed by the variance \( \sigma^2 \).

This concept generalizes naturally to higher dimensions. The \textbf{multivariate normal distribution} is a cornerstone of probabilistic modeling, particularly in the context of regression with Gaussian noise. A random vector \( \mathbf{y} \in \mathbb{R}^N \) is said to follow a multivariable normal distribution with mean \( \boldsymbol{\mu} \in \mathbb{R}^N \) and covariance matrix \( \Sigma \in \mathbb{R}^{N \times N} \), written as
\begin{equation}
    \mathbf{y} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma),
\end{equation}
if its probability density function (PDF) is given by:
\begin{equation}
    p(\mathbf{y}) = \frac{1}{(2\pi)^{N/2} \det(\Sigma)^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{y} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{y} - \boldsymbol{\mu}) \right).
\end{equation}

The mean vector \( \boldsymbol{\mu} \) represents the expected value of the random vector \( \mathbf{y} \), while the covariance matrix \( \Sigma \) captures both the variances of individual components and the correlations between them. The quantity
\begin{equation}
\left\lVert \mathbf{y} - \boldsymbol{\mu} \right\rVert_{\Sigma^{-1}}^2 = (\mathbf{y} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{y} - \boldsymbol{\mu})
\end{equation}
is known as the \textbf{Mahalanobis distance}, which generalizes the notion of distance by incorporating the geometry induced by the covariance matrix. \( \left\lVert \cdot \right\rVert_{\Sigma^{-1}} \) denotes the Mahalanobis norm:
\begin{equation}
\left\lVert \mathbf{v} \right\rVert_{\Sigma^{-1}}^2 = \mathbf{v}^\top \Sigma^{-1} \mathbf{v}.
\end{equation} 
Level sets of constant Mahalanobis distance form \textbf{ellipsoidal contours} in multivariate space, where the orientation and shape of each ellipsoid reflect the underlying covariance structure: longer axes indicate directions of greater variance, while shorter axes correspond to directions of tighter constraint. 

This formulation naturally extends to modeling noisy measurements in regression, where the observations \( Y \) are treated as Gaussian-distributed random variables around a deterministic model prediction. The multivariable normal distribution provides a compact and mathematically convenient way to encode both measurement uncertainty and statistical dependence across different observations. Assuming the noise term \( \eta(X) \) follows a Gaussian distribution with zero mean and known (or estimable) variance, Eqn.~\eqref{eq:noisy_fit} becomes:
\begin{equation}
    Y = f(X, \Lambda) + \eta(X), \qquad \eta(X) \sim \mathcal{N}(0, \Sigma),
\end{equation}
where \( \Sigma \) is the noise covariance matrix. 
Under this assumption, the \textbf{likelihood} of observing data \( Y \) given parameters \( \Lambda \) is:
\begin{equation}
    p(Y \mid X, \Lambda) = \frac{1}{Z} \exp\left( -\frac{1}{2} \left\lVert Y - f(X, \Lambda) \right\rVert_{\Sigma^{-1}}^2 \right),
    \label{likelihood}
\end{equation}
where \( Z \) is the normalization constant.

This probabilistic perspective reveals that minimizing the squared loss function:
\begin{equation}
    \mathcal{L}(X, \Lambda) = \left\lVert Y - f(X, \Lambda) \right\rVert_{\Sigma^{-1}}^2,
    \label{eq:regression_loss}
\end{equation}
is equivalent to performing \textbf{maximum likelihood estimation (MLE)} of \(\Lambda\) under Gaussian noise. 

The loss function in Eqn~\eqref{eq:regression_loss} simply quantifies the disagreement between the measured data \( Y \) and the model prediction \( f(X, \Lambda) \). However, if prior knowledge or assumptions about the parameters \( \Lambda \) are available before observing the data \( Y \), it is appropriate to incorporate this information into the inference process. In such cases, we introduce a \textbf{prior} distribution over \( \Lambda \):
\begin{equation}
    p(\Lambda) = \frac{1}{Z'} \exp\left( -\frac{1}{2} \left\lVert \Lambda - \Lambda_0 \right\rVert_{K^{-1}}^2 \right),
    \label{eq:prior}
\end{equation}

Given the likelihood function \( p(Y \mid X, \Lambda) \) and the prior distribution \( p(\Lambda) \), \textbf{Bayes' theorem} provides a principled way to update our beliefs about the parameters \( \Lambda \) after observing the data \( Y \). The resulting distribution is known as the \textbf{posterior}:
\begin{equation}
    p(\Lambda \mid X, Y) = \frac{p(Y \mid X, \Lambda) \, p(\Lambda)}{p(Y \mid X)},
    \label{eq:bayes}
\end{equation}
where the denominator \( p(Y \mid X) \), known as the marginal likelihood or model evidence, acts as a normalization constant to ensure the posterior integrates to one. It is independent of \( \Lambda \) and does not affect parameter estimation.

A common strategy is to compute the \textbf{maximum a posteriori} (MAP) estimate, which identifies the most probable value of \( \Lambda \) under the posterior distribution:
\begin{equation}
    \Lambda_{\text{MAP}} = \operatorname*{argmax}_\Lambda \, p(\Lambda \mid X, Y).
\end{equation}
Taking the negative logarithm of the posterior and ignoring constants independent of \( \Lambda \), the MAP estimation reduces to the minimization of a regularized loss function:
\begin{equation}
    \mathcal{L}(X, \Lambda) = \left\lVert Y - f(X, \Lambda) \right\rVert_{\Sigma^{-1}}^2 + \left\lVert \Lambda-\Lambda_0 \right\rVert_{K^{-1}}^2.
    \label{eq:map_loss}
\end{equation}

\section{The Role of Data Smoothness in Enhancing Information Throughput}
\label{sec:data_smoothness}

In this section, we explain the benefits of data smoothness from the perspective of Shannon's noisy channel theorem~\cite{Shannon}. In scattering experiments, particularly those with limited counting statistics (e.g., neutron or photon-limited measurements), data continuity provides an underappreciated but powerful source of redundancy that can be harnessed to improve the effective quality of the measurement. This smoothness not only reduces the apparent noise but also enables the construction of priors distributions as discussed in Eqn.~\eqref{eq:prior}, which serve as regularizers in probabilistic inference and enhance the stability and reliability of parameter estimation.

Shannon's noisy channel theorem describes the fundamental trade-off between three quantities:
\begin{itemize}
    \item Message size \( M \): the amount of information being transmitted or extracted,
    \item Repetition rate \( n \): the number of statistically independent samples or trials,
    \item Error probability \( p_{\mathrm{err}} \): the chance of misinterpreting or misestimating the message.
\end{itemize}

Two dual interpretations of this relationship are especially relevant for experimental data:
\begin{enumerate}
    \item For fixed message size \( M \), increasing the number of repetitions \( n \) reduces the error probability \( p_{\mathrm{err}} \).
    \item For fixed error tolerance \( p_{\mathrm{err}} \), increasing \( n \) allows for a larger message size \( M \), i.e., more information can be accessed from the same measurement.
\end{enumerate}

In the context of experimental data, smoothness in the measured function, such as the continuity of \( I(Q) \) in small-angle scattering, acts as a form of implicit repetition. If the function varies slowly over a characteristic length scale \( \lambda \), then nearby measurements \( I(Q_i) \) and \( I(Q_{i+1}) \), taken at spacing \( \delta \ll \lambda \), are statistically correlated and effectively constitute repeated observations of the same underlying value.

Formally, for any \( \epsilon > 0 \), if \( |Q_2 - Q_1| < \lambda \), then \( |I(Q_2) - I(Q_1)| < \epsilon \). This implies that within a local window of size \( \lambda \), we obtain multiple statistically similar observations of the same signal. The effective repetition rate is increased by a factor:
\begin{equation}
    \mu \sim \frac{\lambda}{\delta},
\end{equation}
where \( \delta \) is the sampling interval. This results in a reduction of effective noise and an enhancement in the information that can be extracted from the measurement.
Using this interpretation, the effect of smoothness on measurement precision can be quantified as:
\begin{itemize}
    \item Fixing message size \( M \), the error probability decreases exponentially with the effective repetition:
    \begin{equation}
        p_{\mathrm{err}} \sim \exp(-k \mu), \quad k > 0,
    \end{equation}
    \item Fixing \( p_{\mathrm{err}} \), the log-accessible message size increases with \( \mu \):
    \begin{equation}
        \log M > 0 \sim \mu.
    \end{equation}
\end{itemize}
These relations encapsulate a central insight: smoothness serves as an information amplifier. The statistical performance of a measurement is not determined solely by physical parameters such as flux or detector resolution. Instead, it is fundamentally shaped by the structure of the signal itself. Smoothness introduces an effective repetition in the observed data, enabling inference algorithms to ``see more than what is directly measured.''

Inference algorithms that incorporate smooth priors naturally leverage this structure. By encoding continuity into the prior distribution, these methods align the inference process with the intrinsic regularity of the physical system. This alignment enhances robustness to noise and improves reconstruction accuracy, particularly in low-flux or sparsely sampled experiments.

\section{Applications in Scattering Data Analysis}

To illustrate the practical utility and generality of the proposed probabilistic inference framework, we present three representative case studies drawn from experimental scattering techniques. Each case study is characterized by distinct physical observables, inference models, and experimental data formats. Despite these differences, all cases share a unified structure grounded in Bayesian inference, used to estimate latent physical quantities from noisy or incomplete measurements.

For each scenario, we outline the formulation of the inference problem by specifying:
\begin{itemize}
    \item the model function \( f(X, \Lambda) \) that links latent parameters \( \Lambda \) to observed data,
    \item the definition and physical interpretation of the parameter space \( \Lambda \),
    \item the form of the prior distribution \( p(\Lambda) \), encoding assumptions such as smoothness or sparsity,
    \item the structure of the observation, noise and its covariance \( \boldsymbol{\Sigma} \).
\end{itemize}

We then derive the corresponding maximum a posteriori (MAP) estimator for the inferred quantity, and highlight the computational and physical implications of each formulation.

\subsection{Case I: Small Angle Scattering (SAS)}

The physical quantity measured in small-angle scattering (SAS) experiments is the scattering intensity \( I(Q) \), where \( Q \) denotes the magnitude of the momentum transfer. Since \( I(Q) \) corresponds to the Fourier transform of the autocorrelation function of the spatial distribution of scattering length density (e.g., mass or electron density), SAS provides a powerful means to probe the morphology of materials at nanometer to micrometer length scales. This makes it particularly valuable for investigating complex soft matter systems, polymers, colloids, and biological assemblies.

Despite its utility, SAS techniques suffer from inherent limitations in photon or neutron flux. For neutron scattering, source intensity improvements are extremely challenging: increasing flux typically requires major upgrades to accelerator or reactor-based facilities, which are costly and span decades of development. In the case of X-ray scattering, although laboratory and synchrotron sources can provide relatively high fluxes, the high energy of X-ray photons at equivalent wavelengths imposes practical constraints due to radiation damage and exposure limits. As a result, total exposure time is often restricted, limiting the available data quality and quantity.

These limitations motivate the use of probabilistic inference to enhance the extraction of physical information from sparse or noisy \( I(Q) \) measurements. In this case study, we demonstrate how prior knowledge of material structure and smoothness constraints can be incorporated into a Bayesian framework to infer reliable scattering profiles and underlying structural parameters, even when data are limited.

The objective here is to reconstruct the true scattering intensity \( I(Q) \) from experimentally measured data \( I^{\mathrm{Exp}}(Q) \), which is corrupted by random noise. We assume the measurement model follows an additive Gaussian noise model:
\begin{equation}
    I^{\mathrm{Exp}}(Q) = I(Q) + \eta(Q),
    \label{eq:noisy_fit_SANS}
\end{equation}
where \( \eta(Q) \sim \mathcal{N}(0, \Sigma) \) represents a zero-mean Gaussian noise process with known or estimated covariance structure \( \Sigma \). This defines the likelihood function:
\begin{equation}
    p(I^{\mathrm{Exp}} \mid I) = \frac{1}{Z} \exp\left( -\frac{1}{2} \left\lVert I^{\mathrm{Exp}} - I \right\rVert_{\Sigma^{-1}}^2 \right),
\end{equation}
where \( I \) is the latent noise-free intensity vector evaluated on the same set of \( Q \)-values as the data.

Since the transformation from parameter to observable is direct, we adopt the identity map:
\begin{equation}
    f(Q, \Lambda) = \Lambda(Q),
\end{equation}
and treat the unknown function \( \Lambda(Q) = I(Q) \) as the inference target.

To encode smoothness in \( I(Q) \), we place a multivariable Gaussian prior over \( \Lambda \), assuming that the covariance between two intensity values at \( Q \) and \( Q' \) is given by a kernel function:

\begin{equation}
    \mathrm{Cov}\big(I(Q), I(Q')\big) = K(Q, Q'),
\end{equation}

where \( K(Q, Q') \) is a symmetric, positive-definite function encoding expected spatial correlations. A common choice is the squared exponential (Gaussian) kernel:

\begin{equation}
    K(Q, Q') = \sigma^2 \exp\left( -\frac{(Q - Q')^2}{2\ell^2} \right),
\end{equation}

where \( \sigma^2 \) controls the signal variance and \( \ell \) defines the characteristic correlation length scale. This suppresses sharp fluctuations because such variations would require nearby points to differ significantly, which would be unlikely under a prior that assumes strong correlations (high covariance) between them, allowing robust reconstruction of \( I(Q) \) even in the presence of noise or limited data. 

In the following, we adopt matrix notation for clarity and generality. Let \( \mathbf{I} \in \mathbb{R}^N \) denote the vector of noise-free scattering intensities evaluated at discrete momentum transfer values \( Q_1, \dots, Q_N \), and let \( \mathbf{I}^{\mathrm{Exp}} \in \mathbb{R}^N \) denote the corresponding experimental measurements. We also define \( \mathbf{K} \in \mathbb{R}^{N \times N} \) as the prior covariance matrix and \( \boldsymbol{\Sigma} \in \mathbb{R}^{N \times N} \) as the noise covariance matrix.

\begin{itemize}
    \item \textbf{Prior:} The smoothness of the latent scattering intensity \( \mathbf{I} \) is encoded by a multivariable normal distribution with zero mean and covariance \( \mathbf{K} \):
    \begin{equation}
        p(\mathbf{I}) = \frac{1}{Z'} \exp\left( -\frac{1}{2} \mathbf{I}^\top \mathbf{K}^{-1} \mathbf{I} \right).
    \end{equation}

    \item \textbf{Likelihood:} The measured data \( \mathbf{I}^{\mathrm{Exp}} \) are modeled as Gaussian-distributed around the true intensity with noise covariance \( \boldsymbol{\Sigma} \):
    \begin{equation}
        p(\mathbf{I}^{\mathrm{Exp}} \mid \mathbf{I}) = \frac{1}{Z} \exp\left( -\frac{1}{2} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{I})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{I}) \right).
    \end{equation}

    \item \textbf{Objective:} Maximizing the posterior distribution \( p(\mathbf{I} \mid \mathbf{I}^{\mathrm{Exp}}) \) is equivalent to minimizing the negative log-posterior. This leads to the following objective function:
    \begin{equation}
        \mathcal{L}(\mathbf{I}) = (\mathbf{I}^{\mathrm{Exp}} - \mathbf{I})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{I}) + \mathbf{I}^\top \mathbf{K}^{-1} \mathbf{I}.
    \end{equation}
\end{itemize}
The first term in the objective function enforces agreement with the observed data (data fidelity), while the second term penalizes non-smooth behavior in \( \mathbf{I} \) according to the prior. Minimizing \( \mathcal{L}(\mathbf{I}) \) thus yields the maximum a posteriori (MAP) estimate of the scattering profile.

Owing to the quadratic form of \( \mathcal{L}(\mathbf{I}) \), the MAP estimate can be obtained analytically by solving a linear system. This approach is called \emph{Gaussian Process Regression} (GPR)~\cite{jeffreys1939, Gelman2013, williams1996gaussian, williams1998prediction, GPR} because it stems from the assumption that the unknown function \( \mathbf{I} \) is drawn from a multivariate Gaussian distribution (a Gaussian process prior), while the observation model is linear with additive Gaussian noise. Setting the gradient of the objective with respect to \( \mathbf{I} \) to zero leads to the following equation:
\begin{equation}
    \left( \boldsymbol{\Sigma}^{-1} + \mathbf{K}^{-1} \right) \mathbf{I}^{\mathrm{MAP}} = \boldsymbol{\Sigma}^{-1} \mathbf{I}^{\mathrm{Exp}}.
    \label{zero_gradient}
\end{equation}
Solving this system yields the closed-form solution for the maximum a posteriori (MAP) estimate:
\begin{equation}
    \mathbf{I}^{\mathrm{MAP}} = \left( \boldsymbol{\Sigma}^{-1} + \mathbf{K}^{-1} \right)^{-1} \boldsymbol{\Sigma}^{-1} \mathbf{I}^{\mathrm{Exp}},
    \label{MAP_SANS_forward}
\end{equation}
where \( \mathbf{K} \) is the prior covariance and \( \boldsymbol{\Sigma} \) is the noise covariance matrix.

Alternatively, applying the Woodbury matrix identity
\footnote{
The dual form 
\(
\mathbf{I}^{\mathrm{MAP}} = \mathbf{K} (\boldsymbol{\Sigma} + \mathbf{K})^{-1} \mathbf{I}^{\mathrm{Exp}}
\)
is equivalent to the forward form 
\(
\left( \boldsymbol{\Sigma}^{-1} + \mathbf{K}^{-1} \right)^{-1} \boldsymbol{\Sigma}^{-1} \mathbf{I}^{\mathrm{Exp}}
\)
by the Woodbury identity:
\[
\left( \boldsymbol{\Sigma}^{-1} + \mathbf{K}^{-1} \right)^{-1} = \mathbf{K} - \mathbf{K} (\boldsymbol{\Sigma} + \mathbf{K})^{-1} \mathbf{K}.
\]
Substituting this yields
\[
\begin{aligned}
\mathbf{I}^{\mathrm{MAP}} 
&= \left( \mathbf{K} - \mathbf{K} (\boldsymbol{\Sigma} + \mathbf{K})^{-1} \mathbf{K} \right) \boldsymbol{\Sigma}^{-1} \mathbf{I}^{\mathrm{Exp}} \\
&= \mathbf{K} \left[ \mathbf{I}_N - (\boldsymbol{\Sigma} + \mathbf{K})^{-1} \mathbf{K} \right] \boldsymbol{\Sigma}^{-1} \mathbf{I}^{\mathrm{Exp}} \\
&= \mathbf{K} (\boldsymbol{\Sigma} + \mathbf{K})^{-1} \mathbf{I}^{\mathrm{Exp}},
\end{aligned}
\]
since the relation
\(
\mathbf{I}_N - (\boldsymbol{\Sigma} + \mathbf{K})^{-1} \mathbf{K} = (\boldsymbol{\Sigma} + \mathbf{K})^{-1} \boldsymbol{\Sigma}
\) can also be obtained by the Woodbury formula. \(\mathbf{I}_N\) denotes the identity matrix.
}, the solution can be expressed in a numerically advantageous form:
\begin{equation}
    \mathbf{I}^{\mathrm{MAP}} = \mathbf{K} \left( \boldsymbol{\Sigma} + \mathbf{K} \right)^{-1} \mathbf{I}^{\mathrm{Exp}}.
    \label{MAP_SANS_dual}
\end{equation}

Throughout this manuscript, we refer to Eqn.~\eqref{MAP_SANS_forward} as the \textbf{Forward Form}, as it arises directly from minimizing the objective function via forward variational calculus. In contrast, Eqn.~\eqref{MAP_SANS_dual} is termed the \textbf{Dual Form}, as it shifts computation to the data space through the use of the Woodbury identity. The relative merits of these formulations in terms of computational efficiency, scalability, and interpretability will be discussed in detail in later sections.
Both expressions highlight how the MAP estimate reflects a trade-off between two sources of information: fidelity to the measured data, governed by the noise precision \( \boldsymbol{\Sigma} \), and smoothness imposed by the prior, governed by \( \mathbf{K} \).

This interpretation that prior structure can be leveraged to enhance the inference of latent quantities is not limited to static measurements. It extends naturally to time-resolved scattering experiments, where the underlying dynamics exhibit smooth and often decaying behavior.

\subsection{Case II: Neutron Spin Echo (NSE)}

The NSE experiment measures both spatial and temporal correlations, represented as \( S(Q,t) \). In most liquids and soft matter systems, the intermediate scattering function decays monotonically in time due to relaxation processes. To effectively capture this behavior, we adopt a Prony series expansion~\cite{Tschoegl, Nievergelt2000, Pereyra2010, mauro2018prony}:
\begin{equation}
    S(Q, t) = \sum_{n=1}^N A(Q, \tau_n) \, e^{-t / \tau_n},
    \label{eq:S_Prony}
\end{equation}
where \( A(Q, \tau_n) \) denotes the expansion coefficients and \( \tau_n \) are fixed characteristic relaxation times. In matrix notation, this formulation can be expressed as a linear operation \( \mathbf{C} \mathbf{A} \), where \( \mathbf{C} \in \mathbb{R}^{M \times N} \) is the Prony basis matrix evaluated at measurement times \( t_a \), and \( \mathbf{A} \in \mathbb{R}^{L \times N} \) contains the expansion coefficients at inference positions \( Q_i \).

However, in practical settings, the measurement coordinates \( (Q_a, t_a) \) do not coincide with the coordinates at which inference is desired, \( (Q_i, t_m) \). To reconcile this mismatch, we introduce a row-normalized spatial weight matrix \( \mathbf{W} \in \mathbb{R}^{M \times L} \), which linearly projects the latent coefficients defined at \( Q_i \) to the observation coordinates \( Q_a \). The weights are defined using a Gaussian kernel:
\begin{equation}
    W_{ai} = \frac{\exp\left(-\frac{|Q_a - Q_i|^2}{2\mu^2}\right)}{\sum_{j=1}^{L} \exp\left(-\frac{|Q_a - Q_j|^2}{2\mu^2}\right)},
\end{equation}
where \( \mu \) is a spatial correlation length scale that controls the influence radius of each inference point. The combined spatial-temporal transformation from the coefficient tensor \( A_{in} \) to the observations \( S_a^{\mathrm{Exp}} \) is then expressed through the third-order transformation tensor:
\begin{equation}
    G_{a,in} = C_{an} W_{ai},
    \label{eq:G_definition}
\end{equation}
\begin{equation}
    S_a^{\mathrm{Exp}} = C_{an} W_{ai} A_{in} + \eta_a,
    \label{eq:CWA_eta}
\end{equation}
which maps the \( (Q_i, \tau_n) \)-indexed latent space to observed space \( (Q_a, t_a) \).\footnote{We adopt the \texttt{numpy.einsum} style notation~\cite{harris2020array} throughout this section. This convention provides a flexible and readable representation of tensor contractions, which is particularly useful when describing operations over mixed spatial and temporal coordinates. Unlike traditional Einstein summation notation~\cite{Simmonds1994}, in which repeated indices always imply summation, the \texttt{einsum} style allows repeated indices to represent either summation or element-wise alignment, depending on the context and output dimensionality. This distinction enables compact representations of complex expressions while preserving the underlying structure of the transformation. 
} This compact transformation forms the core of the inference model and enables a flexible yet physically grounded projection of structural dynamics onto the experimental coordinate system.

In this formulation, the model function \( f(X, \Lambda) \) represents the predicted scattering signal at experimental coordinates \( X=(Q_a, t_a) \), given a set of latent expansion coefficients \( \Lambda = A_{in} \), which encode the dynamical properties of the system over a predefined basis of relaxation times \( \tau_n \). The input variable \( X \) consists of the observed spatiotemporal measurement coordinates, while the output \( f(X, \Lambda) \) is obtained by applying the transformation tensor \( G_{a,in} \) to \( A_{in} \). This tensor contraction effectively propagates the latent Prony coefficients, defined at inference coordinates \( (Q_i, \tau_n) \), to the detector coordinates \( (Q_a, t_a) \). Thus, \( f \) encapsulates the forward model linking latent dynamics to measurable quantities, and \( \Lambda \) denotes the parameter space over which inference is performed.

\begin{itemize}
    \item \textbf{Prior:} The latent Prony coefficients \( A_{in} \) are assigned a multivariate Gaussian prior with zero mean and structured covariance \( K_{in,jq} \), which enforces smoothness across spatial coordinates and independence across exponential components:
    \begin{equation}
        p(A) = \frac{1}{Z'} \exp\left( -\frac{1}{2} A_{in} K^{-1}_{in,jq} A_{jq} \right), \qquad \text{with} \quad K_{in,jq} = k_{ij} \delta_{nq}.
    \end{equation}
    The spatial kernel is defined as:
    \begin{equation}
        k_{ij} = \exp\left(-\frac{|Q_i - Q_j|^2}{2\lambda^2} \right),
    \end{equation}
    where \( \lambda \) is a length-scale hyperparameter controlling the decay of spatial correlations.

    \item \textbf{Likelihood:} The measured NSE data \( S_a^{\mathrm{Exp}} \) are modeled as noisy observations of the forward-transformed coefficients through \( G_{a,in} \), with additive Gaussian noise \( \eta_a \sim \mathcal{N}(0, \Sigma_{ab}) \):
    \begin{equation}
        p(S^{\mathrm{Exp}} \mid A) = \frac{1}{Z} \exp\left( -\frac{1}{2} (S_a^{\mathrm{Exp}} - G_{a,in} A_{in}) \Sigma^{-1}_{ab} (S_b^{\mathrm{Exp}} - G_{b,jq} A_{jq}) \right),
    \end{equation}
    where \( \Sigma_{ab} \) is the noise covariance matrix, often taken to be diagonal based on pointwise uncertainty estimates from experimental data.

    \item \textbf{Objective:} Maximizing the posterior distribution \( p(A \mid S^{\mathrm{Exp}}) \) is equivalent to minimizing the negative log-posterior. This leads to the following regularized least-squares objective function:
    \begin{equation}
        \mathcal{L}(A) = (S_a^{\mathrm{Exp}} - G_{a,in} A_{in}) \Sigma^{-1}_{ab} (S_b^{\mathrm{Exp}} - G_{b,jq} A_{jq}) + A_{in} K^{-1}_{in,jq} A_{jq}.
    \end{equation}
\end{itemize}

Following the procedure described in the previous section, the MAP estimation of the Prony coefficients can be expressed as either a forward-form or a dual-form solution, depending on which matrix inversion is more computationally convenient.

\begin{itemize}
    \item \textbf{Forward Form:} The forward form solves the normal equation in coefficient space:
    \begin{equation}
        A^{\mathrm{MAP}}_{in} = \left( G_{a,in} \Sigma^{-1}_{ab} G_{b,jq} + K^{-1}_{in,jq} \right)^{-1}_{in,jq} G_{b,jq} \Sigma^{-1}_{ab} S_a^{\mathrm{Exp}}.
        \label{eq:MAP_forward}
    \end{equation}
    This formulation requires inversion of a matrix in coefficient space of size \( (L \cdot N) \times (L \cdot N) \).

    \item \textbf{Dual Form:} Alternatively, using the Woodbury identity, the MAP estimate can be expressed in dual form, where the matrix size to be inverted is \(M\times M\). This avoids direct inversion in the high-dimensional coefficient space:
    \begin{equation}
        A^{\mathrm{MAP}}_{in} = K_{in,jq} G_{b,jq} \left( G_{a,in} K_{in,jq} G_{b,jq} + \Sigma_{ab} \right)^{-1}_{ab} S_a^{\mathrm{Exp}},
        \label{eq:MAP_dual}
    \end{equation}
\end{itemize}

Other scattering techniques can yield data structures and inference models that resemble those discussed in the NSE case. An example is \textbf{contrast variation small-angle neutron scattering (CV-SANS)}, where multiple scattering profiles \( I(Q) \) are measured from chemically identical samples prepared with different solvent contrasts. In such systems, the total scattering function is expressed as a weighted linear combination of partial structure factors \( S_{ij}(Q) \), determined by the scattering length density (SLD) contrasts \( \Delta \rho_i \) between solutes and solvent. 

For a three-component system in contrast variation SANS, the measurement equation at each scattering vector \( Q_i \) can be expressed as a linear regression model over the contrast index \( \gamma_m \)~\cite{Mayumi2025}:
\begin{equation}
    \mathbf{I}^{\mathrm{Exp}}_{im} = \mathbf{M}_{mn} \mathbf{S}_{ni} + \boldsymbol{\eta}_{im},
    \label{eq:CVSANS}
\end{equation}
where \( \mathbf{I}^{\mathrm{Exp}}_{im} \in \mathbb{R}^{L \times N} \) denotes the measured scattering intensities at \( L \) distinct \( Q_i \) values across \( N \geq 3 \) contrast conditions \( \gamma_m \). The contrast matrix \( \mathbf{M} \in \mathbb{R}^{N \times 3} \) is defined by the known SLD differences:
\begin{equation}
\mathbf{M}_{mn} =
\begin{bmatrix}
(\Delta_1 \rho_1)^2 & (\Delta_1 \rho_2)^2 & 2\Delta_1 \rho_1 \Delta_1 \rho_2 \\
\vdots & \vdots & \vdots \\
(\Delta_N \rho_1)^2 & (\Delta_N \rho_2)^2 & 2\Delta_N \rho_1 \Delta_N \rho_2 \\
\end{bmatrix},
\end{equation}
and \( \mathbf{S}_{ni} \in \mathbb{R}^{3 \times L} \) is the matrix of unknown partial structure factors at each \( Q_i \), with rows corresponding to \( S_{11}(Q_i), S_{22}(Q_i), S_{12}(Q_i) \). The additive noise term \( \boldsymbol{\eta}_{im} \) accounts for measurement uncertainty across combinations of contrast condition and scattering vector.

This formulation mirrors the tensor contraction structure introduced in Eqn.~\eqref{eq:CWA_eta} for the NSE case. Now, assume each row \( \mathbf{S}_n = [\mathbf{S}_{n1}, \dots, \mathbf{S}_{nL}] \in \mathbb{R}^L \) follows a zero-mean multivariate normal distribution with a smooth kernel prior:
\begin{equation}
    p(\mathbf{S}_{ni}) \propto \exp\left( -\frac{1}{2} \mathbf{S}_{ni} \mathbf{K}^{-1}_{ij} \mathbf{S}_{nj} \right),
\end{equation}
where the prior covariance \( \mathbf{K} \in \mathbb{R}^{L \times L} \) is given by:
\begin{equation}
    \mathbf{K}_{ij} = \exp\left( -\frac{(Q_i - Q_j)^2}{2\lambda^2} \right).
\end{equation}
We assume additive Gaussian noise with independent variance for each measurement:
\begin{equation}
    \boldsymbol{\eta}_{im} \sim \mathcal{N}(0, \boldsymbol{\Sigma}_{im}).
\end{equation}

The MAP estimate of the partial structure factors \( \mathbf{S} \in \mathbb{R}^{3 \times L} \) can then be obtained by solving the regularized least-squares problem:
\begin{equation}
    \mathbf{S}^{\mathrm{MAP}} = \left( \mathbf{M}^\top \boldsymbol{\Sigma}^{-1} \mathbf{M} + \mathbf{K}^{-1} \right)^{-1} \mathbf{M}^\top \boldsymbol{\Sigma}^{-1} \mathbf{I}^{\mathrm{Exp}},
\end{equation}
or, equivalently, using the Woodbury matrix identity:
\begin{equation}
    \mathbf{S}^{\mathrm{MAP}} = \mathbf{K} \mathbf{M}^\top \left( \mathbf{M} \mathbf{K} \mathbf{M}^\top + \boldsymbol{\Sigma} \right)^{-1} \mathbf{I}^{\mathrm{Exp}}.
\end{equation}

Such a formulation for indirect inference can be generalized to any scenario where the connection between observations and latent physical quantities is represented by a linear operator. This structure encompasses a broad class of methods that employ basis expansions to regularize noisy measurements.
For instance, consider the case of \textbf{two-dimensional scattering spectra expressed in polar coordinates, \( I(Q, \phi) \)}. At fixed \( Q \), the angular dependence \( I_Q(\phi) \) can be represented by a truncated Fourier series:
\[
I_Q(\phi) = \sum_n A_n(Q) \phi_n(\phi),
\]
where \( \phi_n(\phi) \) denotes the angular basis functions. This decomposition yields a linear transformation structure similar to Eqns.~\eqref{eq:CWA_eta} and~\eqref{eq:CVSANS}, where the coefficients \( A_n(Q) \) are inferred from experimental data. 
In addition to the angular basis, smoothness along the radial \( Q \)-direction can be incorporated through a Gaussian process prior with covariance matrix \( \mathbf{K} \), which promotes continuity and correlation between neighboring \( Q \)-points. This unified framework—combining basis projection with smooth priors—provides a physically grounded and computationally flexible approach to extract latent information from noisy, multidimensional scattering data.

\subsection{Case III: X-ray Photon Correlation Spectroscopy (XPCS)}
The physical quantity measured in X-ray Photon Correlation Spectroscopy (XPCS) is the time autocorrelation of the coherent scattering intensity at a fixed scattering vector \( Q \). For simplicity, we fix \( Q \) and focus on the temporal signal \( I(t) \), representing the time-dependent intensity at the detector. Due to the extremely limited photon counts in typical XPCS experiments, \textbf{direct fitting of \( I(t) \) to a nonlinear model is generally not feasible}. Nonetheless, the time autocorrelation function \( C(\tau) \) is expected to vary smoothly with the lag time \( \tau \), providing a meaningful assumption that can be exploited through Bayesian inference.

Rather than attempting to reconstruct \( C(\tau) \) directly from \( I(t) \), we leverage the Wiener--Khinchin theorem, which relates the autocorrelation function to the power spectral density of the signal. This spectral-domain reformulation enables us to linearize the inference problem and naturally incorporate uncertainty propagation through Fourier transforms.

\begin{itemize}
    \item \textbf{Prior:} The autocorrelation function \( \mathbf{C} = [C(\tau_1), \dots, C(\tau_L)]^\top \in \mathbb{R}^L \) is assumed to be smooth and follows a multivariate Gaussian distribution:
    \begin{equation}
        p(\mathbf{C}) = \frac{1}{Z'} \exp\left( -\frac{1}{2} \mathbf{C}^\top \mathbf{K}^{-1} \mathbf{C} \right),
    \end{equation}
    where the prior covariance matrix \( \mathbf{K} \in \mathbb{R}^{L \times L} \) is defined as
    \begin{equation}
        K_{\ell \ell'} = \sigma^2 \exp\left( -\frac{(\tau_\ell - \tau_{\ell'})^2}{2\lambda^2} \right).
    \end{equation}

    \item \textbf{Likelihood:} The power spectral density \( \mathbf{H}^{\mathrm{Exp}} \in \mathbb{R}^N \) is computed from the noisy time-resolved intensity signal \( \mathbf{I}^{\mathrm{Exp}} \in \mathbb{R}^M \), measured at time points \( t_m \). Define the discrete Fourier transform (DFT) matrix \( \mathbf{F} \in \mathbb{C}^{N \times M} \) by
    \begin{equation}
        \mathbf{F}_{n m} = \exp(-i \omega_n t_m),
    \end{equation}
    where \( \omega_n \) are the angular frequencies of the sampled modes. The frequency-domain signal is
    \begin{equation}
        \tilde{\mathbf{I}} = \mathbf{F} \mathbf{I}^{\mathrm{Exp}}, \qquad H_n^{\mathrm{Exp}} = |\tilde{I}_n|^2,
    \end{equation}
    \begin{equation}
        \mathbf{H}^{\mathrm{Exp}} = \left( \mathbf{F} \mathbf{I}^{\mathrm{Exp}} \right) \odot \left( \mathbf{F} \mathbf{I}^{\mathrm{Exp}} \right)^\dagger,
    \end{equation}
    with \( \tilde{\mathbf{I}} \in \mathbb{C}^N \) the Fourier-transformed signal and \(\odot\) denotes the elementwise (Hadamard) product.

    The time-domain uncertainty is characterized by a diagonal covariance matrix \( \boldsymbol{\Sigma}_I = \mathrm{diag}((\Delta I_1)^2, \dots, (\Delta I_M)^2) \). Propagating this through the DFT, and applying the variance formula for squared modulus of complex Gaussian variables, we approximate the spectral-domain uncertainty by a diagonal covariance matrix \( \boldsymbol{\Sigma}_H \in \mathbb{R}^{N \times N} \):
    \begin{equation}
        [\boldsymbol{\Sigma}_H]_{n n} = 2 \cdot (\mathbf{F} \boldsymbol{\Sigma}_I \mathbf{F}^\dagger)_{n n} \cdot |\tilde{I}_n|^2 + \left( (\mathbf{F} \boldsymbol{\Sigma}_I \mathbf{F}^\dagger)_{n n} \right)^2.
    \end{equation}
    We assume uncorrelated measurement noise in time and negligible correlation across frequencies.

    According to the Wiener--Khinchin theorem, the theoretical power spectral density \( \mathbf{H} \in \mathbb{R}^N \) is the Fourier transform of the autocorrelation function \( \mathbf{C} \in \mathbb{R}^L \). Considering \(\mathbf{C}\) is symmetric, the Direct cosine transform (DCT) matrix is applied: \( \mathcal{F} \in \mathbb{C}^{N \times L} \):
    \begin{equation}
        \mathbf{H} = \mathcal{F} \mathbf{C}, \qquad \mathcal{F}_{n \ell} = \cos(\omega_n \tau_\ell).
    \end{equation}

    Assuming Gaussian noise in the spectral domain, the likelihood becomes:
    \begin{equation}
        p(\mathbf{H}^{\mathrm{Exp}} \mid \mathbf{C}) = \frac{1}{Z} \exp\left( -\frac{1}{2} (\mathbf{H}^{\mathrm{Exp}} - \mathcal{F} \mathbf{C})^\top \boldsymbol{\Sigma}_H^{-1} (\mathbf{H}^{\mathrm{Exp}} - \mathcal{F} \mathbf{C}) \right).
    \end{equation}

    \item \textbf{Objective:} Applying Bayes' theorem, the posterior distribution is
    \begin{equation}
        p(\mathbf{C} \mid \mathbf{H}^{\mathrm{Exp}}) \propto p(\mathbf{H}^{\mathrm{Exp}} \mid \mathbf{C}) \cdot p(\mathbf{C}),
    \end{equation}
    which yields the negative log-posterior:
    \begin{equation}
        \mathcal{L}(\mathbf{C}) =
        \frac{1}{2} \left( \mathbf{H}^{\mathrm{Exp}} - \mathcal{F} \mathbf{C} \right)^\top \boldsymbol{\Sigma}_H^{-1} \left( \mathbf{H}^{\mathrm{Exp}} - \mathcal{F} \mathbf{C} \right)
        + \frac{1}{2} \mathbf{C}^\top \mathbf{K}^{-1} \mathbf{C}.
    \end{equation}
\end{itemize}

Following the procedure mentioned in previous sections, minimizing \( J(\mathbf{C}) \) yields the MAP estimate of the autocorrelation function:
\begin{itemize}
    \item \textbf{Forward Form}:
    \begin{equation}
        \mathbf{C}^{\mathrm{MAP}} = \left( \mathcal{F}^\top \boldsymbol{\Sigma}_H^{-1} \mathcal{F} + \mathbf{K}^{-1} \right)^{-1} \mathcal{F}^\top \boldsymbol{\Sigma}_H^{-1} \mathbf{H}^{\mathrm{Exp}},
    \end{equation}
    or equivalently,
    \item \textbf{Dual Form}:
    \begin{equation}
        \mathbf{C}^{\mathrm{MAP}} = \mathbf{K} \mathcal{F}^\top \left( \mathcal{F} \mathbf{K} \mathcal{F}^\top + \boldsymbol{\Sigma}_H \right)^{-1} \mathbf{H}^{\mathrm{Exp}}.
    \end{equation}
\end{itemize}

This framework enables indirect inference of time autocorrelations from low-flux, high-noise intensity measurements by incorporating spectral-domain uncertainty and leveraging priors for smoothness and regularization.

\section{Inference under Symmetry Assumptions}
In the analysis of two-dimensional (2D) small-angle scattering data, the scattering intensity is represented as a function of the in-plane scattering vector components, \( I(Q_x, Q_y) \). Owing to the inherent rotational symmetries of many soft-matter and biological systems, this intensity is often more naturally expressed in polar coordinates \( (q, \theta) \), where \( q = \sqrt{Q_x^2 + Q_y^2} \) is the magnitude of the scattering vector and \( \theta = \mathrm{atan2}(Q_y, Q_x) \) denotes the azimuthal angle.

To reconstruct \( I(q, \theta) \) from sparse or noisy measurements, we adopt a Gaussian Process Regression (GPR) framework that combines radial smoothness in \( q \) with a localized angular basis expansion in \( \theta \). This formulation allows for accurate denoising and interpolation while preserving the physical symmetries of the underlying scattering signal.

% \subsection{Symmetry-Aware Angular Basis Representation}

We model the angular dependence of the scattering function using a set of basis functions that respect the symmetry \( I(q, \theta) = I(q, \theta + \pi) \). To enforce this, we use Fourier series:
\begin{equation}
    \phi_n(\theta) = \begin{cases}
        1 & \text{if } n = 0 \\
        \cos(2n(\theta-\theta_0)) & \text{for } n = 1, 2, \ldots, N.
    \end{cases}
\end{equation} The center \( \theta_0 \) is determined by the eigenvector of the gyration tensor of scattering intensity.

The angular expansion of the anisotropic component of scattering intensity is written as:
\begin{equation}
    \tilde{I}(q, \theta) = \sum_{n=0}^{N} A_n(q) \phi_n(\theta),
    \label{eq:2}
\end{equation}
where \( A_0(q) \) captures the isotropic component and higher-order \( A_n(q) \) terms encode anisotropic features. 
\begin{equation}
\tilde{I}(q, \theta)=\frac{I(q, \theta)}{I^\mathrm{Iso}(q)}
\end{equation}
\begin{equation}
\Delta\tilde{I}(q, \theta)=\frac{\Delta I(q, \theta)}{I^\mathrm{Iso}(q)}
\end{equation}
and 
\begin{equation}
I^\mathrm{Iso}(q) = 
\frac{1}{2\pi}\int_0^{2\pi} I(q, \theta) dq
\end{equation}

% \subsection{Radial Priors and Spatial Smoothing}
We define a row-normalized spatial weight matrix to map between observed and evaluation coordinates:
\begin{equation}
    W_{ai} = \frac{\exp\left(-\frac{|q_a - q_i|^2}{2\mu^2}\right)}{\sum_{j=1}^{L} \exp\left(-\frac{|q_a - q_j|^2}{2\mu^2}\right)},
    \label{eq:W_ai}
\end{equation}
where \( \mu \) governs the locality of the spatial coupling.

We adopt the following indexing convention: the subscript \( a \) denotes the observed data points, \( i \) indexes the inference grid points along the radial coordinate \( q_i \), and \( n \) labels the angular basis functions. Accordingly, the observed (possibly transformed) intensity is written as
\begin{equation}
\tilde{I}_a^{\mathrm{Exp}} = \tilde{I}^{\mathrm{Exp}}(q_a, \theta_a),
\end{equation}
and the expansion coefficients at the inference points are denoted
\begin{equation}
A_{in} = A_n(q_i).
\end{equation}

The expected intensity at each observation point is then modeled as:
\begin{equation}
    \tilde{I}_a^{\mathrm{Exp}} = W_{ai} \phi_{n} A_{in} + \eta_a,
    \label{eq:I_expansion_w}
\end{equation}
where \( \eta_a \sim \mathcal{N}(0, \Sigma_{ab}) \) represents experimental noise with diagonal covariance:
\begin{equation}
    \Sigma_{ab} = \delta_{ab} (\Delta \tilde{I}_a)^2.
    \label{eq:Sigma_ab}
\end{equation}


To account for radial correlations in expansion coefficients \(A_{in}\), we model the coefficients \( A_{in} \) as smooth functions of \( q_i \) using a radial basis function (RBF) kernel:
\begin{equation}
    k_{ij} = \exp\left(-\frac{(q_i - q_j)^2}{2\lambda^2}\right),
\end{equation}
where \( \lambda \) is a length-scale hyperparameter controlling radial smoothness. Moreover, as empirically the expansion coefficients decays as 
\begin{equation}
    \lvert A_{in}\rvert \sim n^{-2}
\end{equation}
we define a diagonal angular penalty matrix
\begin{equation}
    B_{nq} = n^2\delta_{nq},
\end{equation}
The prior over the coefficients is given by:
\begin{equation}
    A_{in} \sim \mathcal{N}(0, K_{in,jq}), \quad \text{with } K_{in,jq} = k_{ij} B_{nq}.
\end{equation}

% \subsection{Posterior Inference via Gaussian Processes}

We perform Bayesian inference by minimizing the following objective function:
\begin{equation}
    \mathcal{L}(A) = (I_a^{\mathrm{Exp}} - G_{a,in} A_{in}) \Sigma_{ab}^{-1} (I_b^{\mathrm{Exp}} - G_{b,jq} A_{jq}) + (A_{in} K_{in,jq}^{-1}A_{jq}),
\end{equation}
where \( G_{a,in} = W_{ai} \phi_{n} \) is the forward mapping.

The maximum \textit{a posteriori} (MAP) estimation of the coefficients are then obtained via GPR:
\begin{align}
    A_{in}^{\mathrm{MAP}} &= K^G_{b,in} \tilde{\Sigma}^{-1}_{ab} I_b^{\mathrm{Exp}}, 
    % \\
    % K_{in,jq}^{\mathrm{MAP}} &= K_{in,jq} - K^G_{a,in} \tilde{\Sigma}^{-1}_{ab} K^G_{b,jq},
\end{align}
with
\begin{align}
    K^G_{a,in} &= K_{in,jq} G_{a,jq}, \\
    \tilde{\Sigma}_{ab} &= \Sigma_{ab} + G_{a,in} K_{in,jq} G_{b,jq}.
\end{align}

The reconstructed scattering intensity at new coordinates \( (q_i, \theta_i) \) is given by:
\begin{equation}
 I^{\mathrm{MAP}}(q_i, \theta_i) = I^{\mathrm{Iso}}_i\tilde{I}_i^{\mathrm{MAP}},
\end{equation}
where
\begin{equation}
    \tilde{I}_i^{\mathrm{MAP}} = \phi_{in} W_{ij} A_{jn}^{\mathrm{MAP}}.
\end{equation}

The isotropic component at each evaluation coordinate \( i \) can be estimated directly from observed data,
\begin{equation}
    I^{\mathrm{Iso}}_i = W_{ia} I^{\mathrm{Exp}}_a.
\end{equation}

\clearpage
\section{Inference under Irreversible Transformations}
\label{sec:irreversible}

Beyond improving robustness to noise, the probabilistic inference framework is also well-suited for handling \textbf{irreversible transformations}, where the relationship between the observed data and the latent physical quantity is non-invertible or ill-posed. In such scenarios, the forward mapping from the true signal to the measured quantity is often not one-to-one, and the inverse problem lacks a unique solution.

A representative example is the effect of \textbf{instrumental resolution smearing}, where the measured scattering intensity \( I^{\mathrm{Exp}}(Q) \) is related to the true intensity \( I(Q) \) through a convolution with a resolution function \( R(Q, Q') \):
\begin{equation}
    I^{\mathrm{Exp}}(Q) = \int I(Q') \, R(Q, Q') \, dQ'.
    \label{eq:resolution_function}
\end{equation}
Here, \( R(Q, Q') \) is typically modeled as a Gaussian kernel:
\begin{equation}
    R(Q, Q') = \frac{1}{\sqrt{2\pi \Delta_Q(Q)^2}} \exp\left( -\frac{(Q - Q')^2}{2\Delta_Q(Q)^2} \right),
\end{equation}
where \( \Delta_Q(Q) \) is the resolution width, often dependent on the scattering vector \( Q \). This convolution operation smooths or blurs high-frequency components in the true spectrum, making the inversion ill-posed, which means multiple candidate functions \( I(Q) \) may produce the same observed profile.

To make the inversion feasible, we adopt a Bayesian regularization strategy. Specifically, we impose prior knowledge on \( I(Q) \) and incorporate the measurement model into a probabilistic framework:

\begin{itemize}
    \item \textbf{Prior:} A natural assumption is that \( I(Q) \) varies smoothly with \( Q \), which can be captured by a Gaussian process prior:
    \begin{equation}
        p(\mathbf{I}) = \frac{1}{Z'} \exp\left( -\frac{1}{2} \mathbf{I}^\top \mathbf{K}^{-1} \mathbf{I} \right),
    \end{equation}
    where \( \mathbf{I} \in \mathbb{R}^L \) is the vector of intensities evaluated at discrete \( Q_i \) points, and \( \mathbf{K}_{ij} = \sigma^2 \exp\left(-\frac{(Q_i - Q_j)^2}{2\lambda^2} \right) \) is the prior covariance matrix enforcing smoothness.

    \item \textbf{Likelihood:} Discretizing Eqn.~\eqref{eq:resolution_function}, we express the forward model as:
    \begin{equation}
        \mathbf{I}^{\mathrm{Exp}} = \mathbf{R} \mathbf{I} + \boldsymbol{\eta},
    \end{equation}
    where \( \mathbf{I}^{\mathrm{Exp}} \in \mathbb{R}^M \) is the observed intensity vector, and \( \boldsymbol{\eta} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Sigma}) \) models measurement noise with known covariance \( \boldsymbol{\Sigma} \).
    Each entry of the resolution matrix \( \mathbf{R} \in \mathbb{R}^{M \times L} \) is defined by:
    \begin{equation}
        R_{ij} = \frac{\exp\left( -\dfrac{(Q_i - Q_j)^2}{2 \Delta_Q(Q_i)^2} \right)}
        {\sum_{k=1}^{L} \exp\left( -\dfrac{(Q_i - Q_k)^2}{2 \Delta_Q(Q_i)^2} \right)},
    \end{equation}
    where \( Q_i \) denotes the detector coordinate (observation location), \( Q_j \) is the latent coordinate where the true intensity is evaluated, and \( \Delta_Q(Q_i) \) represents the instrument resolution width at point \( Q_i \). This Gaussian kernel formulation ensures each row of \( \mathbf{R} \) is normalized to sum to unity:
    \begin{equation}
        \sum_{j=1}^{L} R_{ij} = 1 \quad \text{for all } i = 1, \dots, M,
    \end{equation}
    thereby preserving total intensity under convolution. Therefore
    \begin{equation}
        p(\mathbf{I}^{\mathrm{Exp}}\mid \mathbf{I}) = 
        \frac{1}{Z}\exp\left( -\frac{1}{2} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{R} \mathbf{I})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{R} \mathbf{I}) \right).
    \end{equation}

    \item \textbf{Objective:} By Bayes' theorem, the posterior over \( \mathbf{I} \) is proportional to the product of the likelihood and prior:
    \begin{equation}
        p(\mathbf{I} \mid \mathbf{I}^{\mathrm{Exp}}) \propto 
        \exp\left( -\frac{1}{2} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{R} \mathbf{I})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{R} \mathbf{I}) \right)
        \cdot \exp\left( -\frac{1}{2} \mathbf{I}^\top \mathbf{K}^{-1} \mathbf{I} \right).
    \end{equation}
    
    The negative log-posterior yields the MAP objective function:
    \begin{equation}
        \mathcal{L}(\mathbf{I}) = 
        (\mathbf{I}^{\mathrm{Exp}} - \mathbf{R} \mathbf{I})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{R} \mathbf{I})
        + \mathbf{I}^\top \mathbf{K}^{-1} \mathbf{I}.
    \end{equation}
\end{itemize}

Minimizing \( \mathcal{L}(\mathbf{I}) \) yields the MAP estimate of the scattering intensity:

\begin{itemize}
    \item \textbf{Forward Form}:
    \begin{equation}
        \mathbf{I}^{\mathrm{MAP}} = \left( \mathbf{R}^\top \boldsymbol{\Sigma}^{-1} \mathbf{R} + \mathbf{K}^{-1} \right)^{-1} \mathbf{R}^\top \boldsymbol{\Sigma}^{-1} \mathbf{I}^{\mathrm{Exp}},
    \end{equation}

    \item \textbf{Dual Form}:
    \begin{equation}
        \mathbf{I}^{\mathrm{MAP}} = \mathbf{K} \mathbf{R}^\top \left( \mathbf{R} \mathbf{K} \mathbf{R}^\top + \boldsymbol{\Sigma} \right)^{-1} \mathbf{I}^{\mathrm{Exp}},
    \end{equation}
\end{itemize}

Alternatively, one can also apply the smooth prior indirectly to the scattering intensity by regularizing the underlying representation. Consider first that \( \mathbf{I} \) can be expressed as a basis expansion in the form:
\begin{equation}
    \mathbf{I} = \mathbf{C} \mathbf{A},
\end{equation}
where \( \mathbf{C} \in \mathbb{R}^{L \times N} \) is a matrix of basis functions evaluated at points \( Q_i \), and \( \mathbf{A} \in \mathbb{R}^N \) is the corresponding coefficient vector. The effect of instrumental resolution smearing on the expanded intensity becomes:
\begin{equation}
    \mathbf{I}^{\mathrm{Exp}} = \mathbf{R} \mathbf{C} \mathbf{A} + \boldsymbol{\eta},
\end{equation}
where \( \mathbf{R} \in \mathbb{R}^{M \times L} \) is the resolution matrix, and \( \boldsymbol{\eta} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Sigma}) \) is Gaussian noise.

To streamline notation, we define the combined forward operator:
\begin{equation}
    \mathbf{G} = \mathbf{R} \mathbf{C},
\end{equation}
so that the observation model becomes:
\begin{equation}
    \mathbf{I}^{\mathrm{Exp}} = \mathbf{G} \mathbf{A} + \boldsymbol{\eta}.
\end{equation}

Assuming the coefficients \( \mathbf{A} \) vary smoothly with respect to \( Q \), we place a Gaussian prior:
\begin{equation}
    p(\mathbf{A}) = \frac{1}{Z'} \exp\left( -\frac{1}{2} \mathbf{A}^\top \mathbf{K}^{-1} \mathbf{A} \right),
\end{equation}
where \( \mathbf{K} \in \mathbb{R}^{N \times N} \) is the prior covariance matrix that enforces smoothness in the coefficient space.

The MAP estimate of \( \mathbf{A} \) is obtained by minimizing the negative log-posterior, resulting in the following closed-form solutions:
\begin{equation}
    \mathbf{A}^{\mathrm{MAP}} = \left( \mathbf{G}^\top \boldsymbol{\Sigma}^{-1} \mathbf{G} + \mathbf{K}^{-1} \right)^{-1} \mathbf{G}^\top \boldsymbol{\Sigma}^{-1} \mathbf{I}^{\mathrm{Exp}},
\end{equation}
\begin{equation}
    \mathbf{A}^{\mathrm{MAP}} = \mathbf{K} \mathbf{G}^\top \left( \mathbf{G} \mathbf{K} \mathbf{G}^\top + \boldsymbol{\Sigma} \right)^{-1} \mathbf{I}^{\mathrm{Exp}}.
\end{equation}

\clearpage
\section{Inference under Chromatic Aberrations}
\label{sec:wavelength_scaling}

In elastic scattering experiments, the detector typically records an intensity that is averaged over a range of incident wavelengths. Traditional instrument design addresses this chromatic dispersion by narrowing the wavelength distribution using energy-selective components, such as rotating choppers or monochromator crystals. However, these selection mechanisms significantly reduce the usable neutron flux by discarding a large fraction of the source spectrum. To better utilize the full intensity of the neutron source, it is therefore important to develop correction algorithms that can account for chromatic aberrations during data analysis. 
In this section, we derive the transformation function that relates the intrinsic scattering intensity to the detector-recorded signal under chromatic aberration. This relationship is expressed in the form of a transformation matrix, which is then integrated into the probabilistic inference framework.

Since the scattering angle \( \theta \) depends on both the wavelength \( \lambda \) and the momentum transfer \( q \), this averaging introduces a spatial rescaling of the observed scattering pattern. We derive a forward model that expresses the measured intensity as a linear transformation of a latent intensity profile defined at a reference wavelength \( \lambda_0 \).

The momentum transfer is given by
\begin{equation}
    q(\theta, \lambda) = \frac{4\pi}{\lambda} \sin\left( \frac{\theta}{2} \right),
    \label{eq:q_theta_lambda}
\end{equation}
where the scattering angle \( \theta \) is determined from the detector displacement from beam center \( d \), and sample-to-detector distance \( L \) via
\begin{equation}
    \theta(d) = \arctan\left( \frac{d}{L} \right).
    \label{eq:theta_d}
\end{equation}
Combining these expressions, the wavelength-dependent momentum transfer at position \( d \) is
\begin{equation}
    q(d, \lambda) = \frac{4\pi}{\lambda} \sin\left( \frac{1}{2} \arctan\left( \frac{d}{L} \right) \right).
    \label{eq:q_d_lambda}
\end{equation}

We assume that for each incident wavelength \( \lambda_i \), the intensity observed at position \( d_k \) corresponds to the intensity of a latent profile evaluated at a rescaled coordinate \( d'_{ik} \) under a fixed reference wavelength \( \lambda_0 \), satisfying
\begin{equation}
    I(d_k, \lambda_i) = I(d'_{ik}, \lambda_0), \quad \text{where} \quad
    d'_{ik} = f_d(d_k, \lambda_i) = q^{-1}(q(d_k, \lambda_i), \lambda_0).
    \label{eq:stretching_relation}
\end{equation}

Given a normalized wavelength distribution \( w(\lambda) \), the measured intensity at detector displacement \( d_k \) is the weighted average of all contributing wavelengths:
\begin{equation}
    I^{\mathrm{Exp}}(d_k) = \sum_{i=1}^N w(\lambda_i) \cdot I(d_k, \lambda_i) = \sum_{i=1}^N w(\lambda_i) \cdot I(d'_{ik}, \lambda_0).
    \label{eq:averaged_I_exp}
\end{equation}

To evaluate \( I(d'_{ik}, \lambda_0) \) on a fixed detector grid \( \{ d_j \}_{j=1}^M \), we apply kernel interpolation using a radial basis function \( \phi(d_j, d'_{ik}) \), defined as:
\begin{equation}
    \phi(d_j, d'_{ik}) = \frac{\exp\left( -\dfrac{(d_j - d'_{ik})^2}{2\mu^2} \right)}{\sum_{j'=1}^M \exp\left( -\dfrac{(d_{j'} - d'_{ik})^2}{2\mu^2} \right)},
    \label{eq:rbf_kernel}
\end{equation}
where \( \mu \) is a kernel width parameter. This yields the approximation:
\begin{equation}
    I(d'_{ik}, \lambda_0) = \sum_{j=1}^M \phi(d_j, d'_{ik}) \cdot I(d_j, \lambda_0).
    \label{eq:interpolation}
\end{equation}

Substituting into Eq.~\eqref{eq:averaged_I_exp}, the full expression for the measured intensity becomes
\begin{equation}
    I^{\mathrm{Exp}}(d_k) = \sum_{i=1}^N \sum_{j=1}^M w(\lambda_i) \cdot \phi(d_j, d'_{ik}) \cdot I(d_j, \lambda_0).
    \label{eq:double_sum}
\end{equation}

We now express this operation in matrix form. Let \( \mathbf{I}^{\mathrm{Exp}} \in \mathbb{R}^M \) denote the observed intensity vector with entries \( I^{\mathrm{Exp}}(d_k) \), and \( \mathbf{I} \in \mathbb{R}^M \) the latent profile at the reference wavelength \( \lambda_0 \). Define the resampling matrix \( \mathbf{R} \in \mathbb{R}^{M \times M} \) by
\begin{equation}
    R_{kj} = \sum_{i=1}^N w(\lambda_i) \cdot \phi(d_j, f_d(d_k, \lambda_i)),
    \label{eq:R_matrix}
\end{equation}
so that the forward model becomes
\begin{equation}
    \mathbf{I}^{\mathrm{Exp}} = \mathbf{R} \mathbf{I}.
    \label{eq:forward_matrix}
\end{equation}

All entries in the matrix \( \mathbf{R} \) are explicitly determined by known parameters: the detector geometry \( \{ d_j \} \), the sample-to-detector distance \( L \), the interpolation kernel width \( \mu \), and the wavelength distribution \( w(\lambda) \). Thus, the averaging process is a deterministic, wavelength-encoded resampling of the latent profile. 

This formulation establishes a clear forward transform model for probabilistic inference to recover the monochromatic scattering profile. Because \( \mathbf{R} \) is independent of the unknown intensity \( \mathbf{I} \), the measurement \( \mathbf{I}^{\mathrm{Exp}} \) can be interpreted as a known linear transformation of \( \mathbf{I} \). 

\begin{itemize}
    \item \textbf{Prior:} We posit that the latent intensity profile \( I(d) \) is a smooth function of detector position. This assumption is implemented via a Gaussian process prior:
    \begin{equation}
        p(\mathbf{I}) = \frac{1}{Z'} \exp\left( -\frac{1}{2} \mathbf{I}^\top \mathbf{K}^{-1} \mathbf{I} \right),
    \end{equation}
    where \( \mathbf{K} \in \mathbb{R}^{M \times M} \) is a covariance matrix defined by a kernel such as
    \begin{equation}
        \mathbf{K}_{jk} = \sigma^2 \exp\left( -\frac{(d_j - d_k)^2}{2\lambda^2} \right),
    \end{equation}
    with hyperparameters \( \sigma \) and \( \lambda \) controlling the amplitude and correlation length of intensity variations.

    \item \textbf{Likelihood:} Assuming Gaussian measurement noise, the observed intensity is modeled as:
    \begin{equation}
        \mathbf{I}^{\mathrm{Exp}} = \mathbf{R} \mathbf{I} + \boldsymbol{\eta}, \quad \boldsymbol{\eta} \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Sigma}),
    \end{equation}
    where \( \boldsymbol{\Sigma} = \mathrm{diag}(\sigma_1^2, \dots, \sigma_M^2) \) encodes the experimental uncertainty. The likelihood function becomes:
    \begin{equation}
        p(\mathbf{I}^{\mathrm{Exp}} \mid \mathbf{I}) = \frac{1}{Z} \exp\left( -\frac{1}{2} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{R} \mathbf{I})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{R} \mathbf{I}) \right).
    \end{equation}

    \item \textbf{Posterior and Objective:} By Bayes’ theorem, the posterior over \( \mathbf{I} \) is proportional to the product of likelihood and prior:
    \begin{equation}
        p(\mathbf{I} \mid \mathbf{I}^{\mathrm{Exp}}) \propto 
        \exp\left( -\frac{1}{2} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{R} \mathbf{I})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{R} \mathbf{I}) \right)
        \exp\left( -\frac{1}{2} \mathbf{I}^\top \mathbf{K}^{-1} \mathbf{I} \right).
    \end{equation}
    Taking the negative log of the posterior yields the MAP objective function:
    \begin{equation}
        \mathcal{L}(\mathbf{I}) = 
        (\mathbf{I}^{\mathrm{Exp}} - \mathbf{R} \mathbf{I})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{I}^{\mathrm{Exp}} - \mathbf{R} \mathbf{I})
        + \mathbf{I}^\top \mathbf{K}^{-1} \mathbf{I}.
    \end{equation}
\end{itemize}

Minimizing this loss gives the MAP estimate of the latent profile:

\begin{itemize}
    \item \textbf{Forward Form:}
    \begin{equation}
        \mathbf{I}^{\mathrm{MAP}} = \left( \mathbf{R}^\top \boldsymbol{\Sigma}^{-1} \mathbf{R} + \mathbf{K}^{-1} \right)^{-1} \mathbf{R}^\top \boldsymbol{\Sigma}^{-1} \mathbf{I}^{\mathrm{Exp}},
    \end{equation}

    \item \textbf{Dual Form:}
    \begin{equation}
        \mathbf{I}^{\mathrm{MAP}} = \mathbf{K} \mathbf{R}^\top \left( \mathbf{R} \mathbf{K} \mathbf{R}^\top + \boldsymbol{\Sigma} \right)^{-1} \mathbf{I}^{\mathrm{Exp}}.
    \end{equation}
\end{itemize}

The MAP estimate \( \mathbf{I}^{\mathrm{MAP}} \) can now be computed from the measured intensity \( \mathbf{I}^{\mathrm{Exp}} \), the associated uncertainty covariance \( \boldsymbol{\Sigma} \), and the known wavelength distribution \( w(\lambda) \).

% This approach allows wavelength effects to be cleanly deconvolved through prior-informed inference. Alternatively, the latent intensity \( \mathbf{I} \) can be expressed in a parametric form using basis functions:
% \begin{equation}
%     \mathbf{I} = \mathbf{C} \mathbf{A},
% \end{equation}
% where \( \mathbf{C} \in \mathbb{R}^{M \times N} \) is the basis matrix and \( \mathbf{A} \in \mathbb{R}^N \) the coefficient vector. The measurement model becomes:
% \begin{equation}
%     \mathbf{I}^{\mathrm{Exp}} = \mathbf{R} \mathbf{C} \mathbf{A} + \boldsymbol{\eta},
% \end{equation}
% with the composite operator \( \mathbf{G} = \mathbf{R} \mathbf{C} \). Assuming a Gaussian prior on \( \mathbf{A} \),
% \begin{equation}
%     p(\mathbf{A}) = \frac{1}{Z'} \exp\left( -\frac{1}{2} \mathbf{A}^\top \mathbf{K}^{-1} \mathbf{A} \right),
% \end{equation}
% the MAP solution becomes:
% \begin{equation}
%     \mathbf{A}^{\mathrm{MAP}} = \left( \mathbf{G}^\top \boldsymbol{\Sigma}^{-1} \mathbf{G} + \mathbf{K}^{-1} \right)^{-1} \mathbf{G}^\top \boldsymbol{\Sigma}^{-1} \mathbf{I}^{\mathrm{Exp}},
% \end{equation}
% \begin{equation}
%     \mathbf{A}^{\mathrm{MAP}} = \mathbf{K} \mathbf{G}^\top \left( \mathbf{G} \mathbf{K} \mathbf{G}^\top + \boldsymbol{\Sigma} \right)^{-1} \mathbf{I}^{\mathrm{Exp}}.
% \end{equation}

% \subsection*{Small-Angle Approximation (Optional)}

% For small scattering angles \( \theta \ll 1 \), we have \( \sin(\theta/2) \approx \theta/2 \) and \( \arctan(y/L) \approx y/L \). Substituting into Eq.~\eqref{eq:q_y_lambda}, we obtain the linearized form:
% \begin{equation}
%     q(y, \lambda) \approx \frac{2\pi y}{\lambda L}.
% \end{equation}
% This yields the simplified inverse transformation:
% \[
% q^{-1}(q, \lambda_0) \approx \frac{q \lambda_0 L}{2\pi},
% \quad \Rightarrow \quad
% q^{-1}(q(y, \lambda_i), \lambda_0) \approx y \cdot \frac{\lambda_i}{\lambda_0}.
% \]
% Substituting into Eq.~\eqref{eq:I_exp_discrete}, we recover the approximate rescaling relation:
% \begin{equation}
%     I^{\mathrm{Exp}}(y) \approx \sum_{i=1}^N w_i \cdot I\left( y \cdot \frac{\lambda_i}{\lambda_0}, \lambda_0 \right),
%     \label{eq:I_exp_scaled_sum}
% \end{equation}
% which expresses the observed profile as a weighted average over geometrically stretched versions of the reference intensity profile.


\clearpage
\section{Inference under Extremely Low Detector Count}
\label{sec:poisson}

In scattering experiments with weakly scattering samples or limited exposure times, the number of detected scattering events per detector bin is often small. In such low-count regimes, the assumption of additive Gaussian noise becomes invalid, and the measurement noise is better modeled by a \textbf{Poisson distribution}, which captures the discrete, non-negative nature of the observed counts.

Let the measured data be \( \mathbf{y} = [y_1, y_2, \dots, y_M]^\top \), where each \( y_i \in \mathbb{N} \) represents the number of scattering events at detector position \( Q_i \). The expected number of counts is modeled as:
\begin{equation}
    \boldsymbol{\mu} = \mathbf{G} \mathbf{A},
\end{equation}
where \( \mathbf{G} \in \mathbb{R}^{M \times N} \) is the forward operator (encoding instrumental resolution and basis representation), and \( \mathbf{A} \in \mathbb{R}^N \) is the latent coefficient vector to be inferred.

\begin{itemize}
    \item \textbf{Prior:} A Gaussian prior is placed on \( \mathbf{A} \), promoting smoothness via a positive-definite covariance matrix \( \mathbf{K} \in \mathbb{R}^{N \times N} \):
    \begin{equation}
        p(\mathbf{A}) = \frac{1}{Z'} \exp\left( -\frac{1}{2} \mathbf{A}^\top \mathbf{K}^{-1} \mathbf{A} \right).
    \end{equation}

    \item \textbf{Likelihood:} Each observation \( y_i \) is modeled as an independent Poisson random variable with mean \( \mu_i = [\mathbf{G} \mathbf{A}]_i \):
    \begin{equation}
        p(y_i \mid \mu_i) = \frac{\mu_i^{y_i} e^{-\mu_i}}{y_i!},
    \end{equation}
    \begin{equation}
        p(\mathbf{y} \mid \mathbf{A}) = \prod_{i=1}^M \frac{[\mathbf{G} \mathbf{A}]_i^{y_i} e^{-[\mathbf{G} \mathbf{A}]_i}}{y_i!}.
    \end{equation}

    \item \textbf{MAP Objective:} Maximizing the posterior \( p(\mathbf{A} \mid \mathbf{y}) \propto p(\mathbf{y} \mid \mathbf{A}) p(\mathbf{A}) \) is equivalent to minimizing the negative log-posterior:
    \begin{equation}
        \mathcal{L}(\mathbf{A}) = \sum_{i=1}^M \left( [\mathbf{G} \mathbf{A}]_i - y_i \log([\mathbf{G} \mathbf{A}]_i) \right) + \frac{1}{2} \mathbf{A}^\top \mathbf{K}^{-1} \mathbf{A} + \text{const},
    \end{equation}
    where the constant includes terms like \( \log(y_i!) \) that do not depend on \( \mathbf{A} \).
\end{itemize}

\subsection{Minimization Algorithm}
Since the gradient of the MAP objective can be derived explicitly:
\begin{equation}
    \nabla_{\mathbf{A}} \mathcal{L}(\mathbf{A}) = 
    \mathbf{G}^\top \left( \mathbf{1} - \frac{\mathbf{y}}{\mathbf{G} \mathbf{A}} \right) + \mathbf{K}^{-1} \mathbf{A},
\end{equation}
gradient-based iterative minimization algorithms, such as gradient descent or quasi-Newton methods, can be used to find the optimal \( \mathbf{A} \).

A suitable initial guess can be obtained from the Gaussian approximation:
\begin{equation}
    \mathbf{A}^{(1)} = \left( \mathbf{G}^\top \boldsymbol{\Sigma}^{-1} \mathbf{G} + \mathbf{K}^{-1} \right)^{-1} \mathbf{G}^\top \boldsymbol{\Sigma}^{-1} \mathbf{y},
\end{equation}
\begin{equation}
    \boldsymbol{\Sigma} = \mathrm{diag}(\mathbf{y} + \epsilon),
\end{equation}
where \( \epsilon > 0 \) is a small constant added to avoid singularities when \( y_i = 0 \).

We then proceed iteratively using \textbf{gradient descent} to minimize the MAP objective.
At each iteration \( j = 1, 2, \ldots \), we update \( \mathbf{A} \) according to:
\begin{equation}
    \mathbf{A}^{(j+1)} = \mathbf{A}^{(j)} - \alpha_j \nabla_{\mathbf{A}} \mathcal{L}(\mathbf{A}^{(j)}),
\end{equation}
where \( \alpha_j > 0 \) is the step size.

Alternatively, a change of variable can be applied to prevent invalid evaluations of the Poisson log-likelihood when the predicted intensities \( \boldsymbol{\mu} = \mathbf{G} \mathbf{A} \) contain non-positive values. Specifically, we introduce an unconstrained variable \( \mathbf{u} \in \mathbb{R}^M \) and define:
\begin{equation}
    \mathbf{G} \mathbf{A} = \exp(\mathbf{u}),
\end{equation}
where the exponential is applied element-wise. This reparameterization ensures that the expected counts \( \mu_i = [\mathbf{G} \mathbf{A}]_i \) remain strictly positive for all \( i \), as required by the Poisson likelihood.

Given \( \mathbf{G} \mathbf{A} = \exp(\mathbf{u}) \), a consistent solution for \( \mathbf{A} \) is obtained:
\begin{equation}
    \mathbf{A}(\mathbf{u}) = \mathbf{G}^\dagger \exp(\mathbf{u}).
\end{equation}
where \( \mathbf{G}^\dagger = \left( \mathbf{G}^\top \mathbf{G} \right)^{-1} \mathbf{G}^\top \) denotes the Moore--Penrose pseudoinverse of \( \mathbf{G} \).
By the chain rule, the gradient with respect to \( \mathbf{u} \) is given by
\begin{equation}
\nabla_{\mathbf{u}} \mathcal{L}(\mathbf{u}) =
\exp(\mathbf{u}) - \mathbf{y}
+ \mathrm{diag}(\exp(\mathbf{u})) \cdot (\mathbf{G}^\dagger)^\top \mathbf{K}^{-1} \mathbf{G}^\dagger \exp(\mathbf{u})
\end{equation}
Gradient-based optimization can then proceed on the variable \( \mathbf{u} \).

\clearpage

\begin{table}[h!]
\renewcommand{\arraystretch}{1.5}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccccc}

\shortstack{\textbf{Experimental} \\ \textbf{Technique}} & \textbf{Observable} & \textbf{Likelihood} & \shortstack{\textbf{Inference} \\ \textbf{Target}} & \textbf{Prior} \\
\hline
\hline
SANS & \( I^{\mathrm{Exp}}(Q) \pm \Delta I \) & \( \mathcal{N}(I^{\mathrm{Exp}} \mid I, \Sigma) \) & Scattering profile \( I(Q) \) & Smooth in \( Q \) \\
\hline
NSE & \( S^{\mathrm{Exp}}(Q, t) \pm \Delta S \) & \( \mathcal{N}(S^{\mathrm{Exp}} \mid G A, \Sigma) \) & Prony coefficients \( A(Q, \tau) \) & Smooth in \( Q \) \\
\hline
CVSANS & \( I^{\mathrm{Exp}}(Q, \gamma) \pm \Delta I \) & \( \mathcal{N}(I^{\mathrm{Exp}} \mid M S, \Sigma) \) & Partial structure factors \( S_{n}(Q) \) & Smooth in \( Q \) \\
\hline
Desmearing & \( I^{\mathrm{Exp}}(Q) \pm \Delta I \) & \( \mathcal{N}(I^{\mathrm{Exp}} \mid R I, \Sigma) \) & Resolution-corrected profile \( I(Q) \) & Smooth in \( Q \) \\
\hline
2D Scattering & \( I^{\mathrm{Exp}}(Q, \theta) \pm \Delta I \) & \( \mathcal{N}(I^{\mathrm{Exp}} \mid \Phi A, \Sigma) \) & Fourier series coefficients \( A_n(Q) \) & Smooth in \( Q \) \\
\hline
XPCS & \( I^{\mathrm{Exp}}(t) \pm \Delta I \) & \( \mathcal{N}(H^{\mathrm{Exp}} \mid \mathcal{F} C, \Sigma_H) \) & Autocorrelation \( C(\tau) \) & Smooth in \( \tau \) \\
\hline
\end{tabular}%
}
\caption{Summary of probabilistic inference formulations for various scattering techniques, highlighting the observable quantities, statistical likelihoods, inferred physical parameters, and prior assumptions. Measurement uncertainties are explicitly incorporated in the observed data.}
\label{tab:inference_summary}
\end{table}


\bibliographystyle{unsrt}
\bibliography{ref}

\end{document}
